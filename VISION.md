# Spec Engineering: A Technical Vision

## The Problem

Software development with AI has a specification problem. The current dominant workflow is:

1. Human describes intent in natural language
2. AI generates code
3. Human reviews code for correctness
4. Repeat until acceptable

This workflow fails at scale because **natural language is lossy, code review is expensive, and there is no verifiable artifact that captures intent independently of implementation**. The result is "vibecoding" -- software that appears correct but has no formal relationship to what was actually requested.

The emerging field of Spec-Driven Development (SDD) addresses part of this: tools like AWS Kiro, GitHub Spec Kit, and OpenSpec introduce structured specification files that precede code generation. But these approaches treat specs as static documents -- planning artifacts consumed by AI, not executable contracts verified against implementations. They solve the "generate from intent" problem without solving the "verify against intent" problem.

Meanwhile, formal verification research (Kleppmann 2025, JetBrains "vericoding" benchmarks 2025) demonstrates that LLMs can generate provably correct code when given formal specifications -- with Dafny verification success rates improving from 68% to 96% in a single year. And Robert C. Martin's Empire-2025 experiment independently validated that dual-stream testing (behavioral acceptance tests + structural unit tests) dramatically improves AI code quality by constraining the solution space from both ends.

**Spec Engineering** unifies these threads into a single methodology and toolchain: a progressive refinement pipeline that transforms human intent into verified, executable behavioral specifications, which then serve as the primary artifact from which code is derived and against which code is verified.

---

## Prior Art and Landscape

### What Exists

**Spec-Driven IDEs and Frameworks**
- **AWS Kiro** (July 2025): VS Code fork structuring AI coding around `requirements.md`, `design.md`, `tasks.md`. High-formality, enterprise-oriented. No behavioral verification.
- **GitHub Spec Kit** (September 2025): Open-source four-phase toolkit (Specify → Plan → Tasks → Implement). Works across AI tools. Templated but not executable.
- **OpenSpec by Fission-AI** (2025): "Brownfield-first" strategy with delta specs for evolving codebases. Supports 21 AI tools. Strongest brownfield story but no formal verification.
- **GitHub Copilot Workspace** (sunset May 2025): The original spec-driven environment. Task → Specification → Plan → Implementation. Capabilities absorbed into Copilot agent mode.

**BDD/ATDD + AI**
- Traditional BDD tools (Cucumber, Behave, Reqnroll) have not shipped native AI code generation from Gherkin specs. Integration happens via external LLM calls, not framework features.
- AI trust in generated tests has dropped to 29% (from 40%), making human-in-the-loop validation critical.
- No major BDD framework has a built-in "generate implementation from behavioral specs" capability. This is the gap.

**Claude Code Plugins**
- **TDG** (chanwit): Test-Driven Generation implementing Red-Green-Refactor loops.
- **TDD Guard** (nizos): Enforcement plugin blocking test skipping.
- **Superpowers**: 29k+ stars, TDD + subagents + planning.
- **andlaf-ak/claude-code-agents**: Full ATDD pipeline with problem-analyst → user-story-writer → atdd-developer → clean-coder agents.
- None integrate formal specification, state machine modeling, and acceptance testing into a unified pipeline.

**Formal Verification + LLMs**
- **SpecGen** (ICSE 2025): Generates JML specs (pre/postconditions, loop invariants) via LLM. 279/385 programs verified.
- **Self-Spec** (OpenReview 2025): LLMs design their own spec schemas before coding. GPT-4o +5%, Claude 3.7 +2% on HumanEval.
- **Vericoding Benchmark** (Sep 2025): 12,504 formal specs across Dafny (82% success), Verus/Rust (44%), Lean (27%).
- **NeuroInv** (Dec 2025): Neurosymbolic loop invariant generation. 99.5% success rate.
- **Mercari GEARS** (Nov 2025): Production TLA+ specs generated by LLM per code change, combined with model-guided fuzz testing. Found real production bugs.
- **TLA+ proof automation** (Dec 2025): Active research frontier with Claude, DeepSeek, Gemini, GPT-5 evaluated on proof tasks.

**State Machine Extraction**
- **FlowFSM** (Jul 2025): Agentic FSM extraction from RFC documents using prompt chaining. Built on CrewAI.
- **ProtocolGPT** (2024): RAG-enhanced state machine inference from protocol implementations. 90%+ precision.
- **XState/Stately.ai**: Dominant statecharts library. Stately Agent bridges state machines and AI agents.
- **SysML ↔ Gherkin** (Springer 2024): Demonstrated that GWT structure maps naturally to state machine transitions.
- **Gap**: No tool extracts behavioral state machines from arbitrary application code or from GWT specs.

**Contract-Driven AI Development (C-DAD)** (Nov 2025): Proposes living runtime contracts rather than static specs. Still conceptual.

### The Core Insight from Uncle Bob's Empire Experiment

Robert C. Martin's six-week experiment building Empire-2025 with Claude produced several findings that this vision builds upon:

1. **The Perverse Incentive**: AI fills behavioral specs with implementation details. `GIVEN the UserService has an empty userRepository` instead of `GIVEN there are no registered users`. This is the same failure that plagued Cucumber for years, now accelerated by AI.

2. **Dual Test Streams**: Acceptance tests define WHAT (behavior). Unit tests define HOW (structure). Both constrain the AI simultaneously. "The two different streams of tests cause Claude to think much more deeply about the structure of the code. It can't just willy-nilly plop code around and write a unit test for it."

3. **The Parser/Generator Architecture**: Not Cucumber. A custom parser/generator pair that understands both a domain-specific GWT syntax and the codebase internals. It produces complete, runnable tests -- not stubs requiring glue code.

4. **Spec Portability**: Same GWT specs → implementations in Java, C, Clojure, Ruby, Rust, JavaScript. The specs are the product. The code is generated.

5. **Workflow Separation**: Two Claude instances -- one for planning (read-only access to code), one for implementation. Git worktrees prevent cross-contamination. Rule: "Never execute a plan until it has been marked reviewed."

6. **Architecture Blindness**: "Claude codes faster than I do, by a significant factor. Claude can hold more details in its 'mind' than I can. But Claude cannot hold the big picture in its mind."

This last point is critical. It implies that the specification -- the big picture -- must be authored and maintained by humans (possibly with AI assistance), and the AI's role is to implement faithfully within the constraints that specifications impose.

---

## The Spec Engineering Pipeline

### Overview

Spec Engineering is a progressive refinement pipeline designed for AI-assisted development. Each phase increases formality and decreases ambiguity, while simultaneously building richer associative context in the AI's latent state.

```
Phase 1: Research          → Broad understanding (human + AI)
Phase 2: Vision            → Directional intent (human + AI → document)
Phase 3: Behavioral Specs  → Precise GWT specifications (human + AI → executable artifact)
Phase 4: State Extraction  → Implicit state machine made explicit (AI → verified graph)
Phase 5: Gap Analysis      → Completeness checking (AI → human review)
Phase 6: Implementation    → Code generation constrained by dual test streams (AI → verified code)
Phase 7: Verification      → Formal property checking (AI + tooling → proofs)
```

### Phase 1: Collaborative Research

**Purpose**: Build shared context between human and AI. The document produced is secondary; the associative links formed in the AI's latent state are the primary output.

**Activities**:
- Domain exploration: existing solutions, competitive landscape, academic precedent
- Technical constraints: platform limitations, performance requirements, integration points
- User intent clarification: what problem is being solved, for whom, why now

**Artifact**: Research notes (consumed by subsequent phases, not externally published)

**Key Property**: Must occur in the same context window as subsequent phases. The softmax write from research → document loses associative richness. The latent state carries information that no document can fully capture.

### Phase 2: Vision Synthesis

**Purpose**: Crystallize research into directional intent. This phase deepens the AI's internal representations while producing a human-readable architectural overview.

**Artifact**: Vision document (this document is an example)

**Key Property**: The process of writing the vision document forces the AI to form cross-references between domain concepts, technical constraints, and user intent. These cross-references inform spec quality downstream.

### Phase 3: Behavioral Specification (GWT)

**Purpose**: Transform directional intent into precise, machine-verifiable behavioral constraints.

**Format**: Given/When/Then specifications at the behavioral level -- describing what users observe, not how systems implement.

```
;===============================================================
; User can register with email and password.
;===============================================================
GIVEN no registered users.

WHEN a user registers with email "bob@example.com" and password "secret123".

THEN there is 1 registered user.
THEN the user "bob@example.com" can log in.
```

**The Spec Guardian**: An active agent that monitors GWT specs for implementation detail leakage. It enforces the behavioral abstraction barrier:

| Rejected (implementation detail)              | Accepted (behavioral)                |
|-----------------------------------------------|--------------------------------------|
| GIVEN the UserService has empty userRepository | GIVEN no registered users            |
| THEN the database contains 1 row              | THEN there is 1 registered user      |
| WHEN the POST /api/users endpoint is called   | WHEN a user registers                |
| GIVEN the Redis cache is empty                | GIVEN no cached sessions             |

**Key Property**: GWT specs are dramatically more token-efficient than prose specs while preserving more behavioral information. A 2000-word spec section compresses to ~10 GWT scenarios that are actually more precise. This means more specification fits in the same context window.

**Portability**: GWT specs are language-independent. The same spec file drives implementations in any language. The spec is the product; the code is derived.

### Phase 4: State Machine Extraction

**Purpose**: Make the implicit state machine in the GWT specs explicit. GWT scenarios describe states (GIVEN), transitions (WHEN), and postconditions (THEN). A collection of scenarios is a partial state machine. "Partial" is where bugs live.

**Process**:
1. Parse all GWT scenarios into `(precondition_state, event, postcondition_state)` triples
2. Construct a directed graph of states and transitions
3. Identify all unique states (GIVEN clauses and THEN clauses that become future GIVENs)
4. Map transitions between states

**Output**: An explicit state machine graph with:
- All states enumerated
- All transitions labeled with events
- Entry points (initial states)
- Terminal states (if any)

**Technical Approach**: This is structured analysis, not open-ended reasoning. The AI extracts and constructs rather than reasons about the state space. This plays to LLM strengths (pattern matching, structured output) rather than weaknesses (combinatorial enumeration).

**Prior Art**: The SysML ↔ Gherkin mapping (Springer 2024) demonstrated this is feasible. FlowFSM and ProtocolGPT show that LLM-based state machine extraction achieves 90%+ precision on protocol specifications.

### Phase 5: Gap Analysis

**Purpose**: Identify what the specs don't cover. The state machine from Phase 4 makes incompleteness visible.

**Automated Checks**:
- **Dead-end states**: States with no outbound transitions (is this intentional?)
- **Unreachable states**: States with no inbound transitions from the initial state
- **Missing error transitions**: What happens when an event occurs in a state that doesn't handle it?
- **Contradictory postconditions**: Two scenarios that produce conflicting THEN clauses from the same GIVEN+WHEN
- **Missing negative scenarios**: What should NOT happen? (LLMs are particularly good at generating adversarial interpretations)
- **Temporal property gaps**: Liveness ("eventually X happens") and safety ("Y never happens") properties that BDD scenarios inherently miss
- **Concurrency gaps**: What if two events occur simultaneously?

**Human Review**: The gap analysis produces a checklist for human triage. Some gaps are genuine missing specs; some are intentionally out of scope. The human decides; the AI presents.

**Research Support**: Property-based testing with LLMs (FSE 2025) shows 23-37% improvement over traditional TDD when LLMs generate adversarial test properties. This same technique applies to gap detection.

### Phase 6: Implementation

**Purpose**: Generate code constrained by both behavioral specs (WHAT) and structural tests (HOW).

**Dual Test Stream Architecture** (following Martin's Empire-2025 methodology):

```
GWT Behavioral Specs (acceptance tests)
    ↓
    Parser → Intermediate Representation → Test Generator
    ↓
    Executable Acceptance Tests (verify WHAT)

    +

    Unit Tests (verify HOW)
    ↓
    Both must pass simultaneously
```

**The Parser/Generator**: Not Cucumber. A project-specific parser/generator pair that:
1. Understands the domain-specific GWT syntax
2. Has deep knowledge of the codebase structure
3. Produces complete, runnable tests (not stubs requiring manual glue code)
4. Generates an intermediate representation (IR) that can be inspected and debugged

**Why not Cucumber?** Cucumber maps step definitions to code via regex. This creates a maintenance burden and a glue-code layer that the AI must also understand and maintain. Martin's approach bypasses this: the generator has full codebase awareness and produces tests that directly exercise internal APIs. The GWT syntax remains human-readable; the generated tests are machine-generated and not meant for human editing.

**Implementation Constraints**:
- Acceptance tests constrain the behavioral contract (prevents the AI from changing what the system does)
- Unit tests constrain the structural approach (prevents the AI from satisfying acceptance tests via hacky shortcuts)
- Both streams must pass simultaneously before any change is accepted

### Phase 7: Verification (Progressive Formality)

**Purpose**: Increase confidence in correctness beyond test passing.

**Tier 1 -- Behavioral Verification** (always):
- All GWT acceptance tests pass
- All unit tests pass
- State machine graph has no unintentional dead ends or unreachable states

**Tier 2 -- Property-Based Verification** (recommended for business logic):
- LLM-generated property-based tests (QuickCheck-style) exercise invariants
- Properties derived from GWT specs: "for all valid registrations, the user count increases by exactly 1"
- Automated input generation explores edge cases that specific scenarios miss

**Tier 3 -- Lightweight Formal Verification** (for critical paths):
- Extract pre/postconditions from GWT specs as Dafny or JML annotations
- LLM generates proof obligations; formal verifier checks them
- Focus on critical state transitions (financial calculations, access control, data integrity)
- Current feasibility: 82% success rate on Dafny (Vericoding Benchmark 2025), improving rapidly

**Tier 4 -- Full Formal Verification** (for safety-critical systems):
- TLA+ specifications for concurrent/distributed behavior
- Alloy models for structural constraints
- Model checking for temporal properties
- Currently requires human expertise; LLM assistance is emerging (Mercari GEARS, TLA+ proof automation)

---

## Architecture

### Core Components

```
┌─────────────────────────────────────────────────────┐
│                  Spec Engineering CLI                 │
│                                                       │
│  ┌─────────┐  ┌──────────┐  ┌───────────────────┐  │
│  │ Research │→ │  Vision  │→ │   GWT Authoring   │  │
│  │  Agent   │  │  Agent   │  │   + Spec Guardian │  │
│  └─────────┘  └──────────┘  └───────────────────┘  │
│                                       │              │
│                              ┌────────▼────────┐    │
│                              │  State Machine  │    │
│                              │   Extractor     │    │
│                              └────────┬────────┘    │
│                                       │              │
│                              ┌────────▼────────┐    │
│                              │  Gap Analyzer   │    │
│                              └────────┬────────┘    │
│                                       │              │
│                    ┌──────────────────┼──────────┐  │
│                    │                  │           │  │
│           ┌────────▼────────┐ ┌──────▼──────┐   │  │
│           │ Parser/Generator│ │  Unit Test  │   │  │
│           │   Pipeline      │ │  Generator  │   │  │
│           └────────┬────────┘ └──────┬──────┘   │  │
│                    │                  │           │  │
│           ┌────────▼──────────────────▼────────┐ │  │
│           │      Verification Engine           │ │  │
│           │  (behavioral → property → formal)  │ │  │
│           └────────────────────────────────────┘ │  │
│                                                    │  │
└────────────────────────────────────────────────────┘
```

### The Spec Guardian Agent

A persistent agent that monitors all GWT spec files and rejects implementation detail leakage. This addresses Uncle Bob's "perverse incentive" problem.

**Detection heuristics**:
- References to class/module/function names from the codebase
- Database/storage terminology (table, row, column, cache, queue)
- API/protocol terminology (endpoint, request, response, status code)
- Framework-specific terminology (service, repository, controller, middleware)
- Internal data structure references (array, hash, map, list -- when describing domain concepts)

**Correction approach**: When leakage is detected, the guardian rewrites the spec in behavioral language and presents both versions for human approval. It does not auto-fix; it proposes.

### The Parser/Generator Pipeline

For each target project, the pipeline is bootstrapped:

1. **Analysis**: The AI examines the project's language, test framework, and module structure
2. **Parser Generation**: A parser is generated that understands the project-specific GWT syntax and produces a structured IR (JSON, EDN, or similar)
3. **Generator Generation**: A code generator is generated that transforms IR into executable tests using the project's test framework and internal APIs
4. **Validation**: The pipeline is tested against a reference spec to ensure round-trip correctness

This is a one-time setup per project. Once the pipeline exists, new specs automatically produce new tests.

**Language Support**: The GWT specs are language-independent. Only the generator changes per language/framework. Same specs → tests in Python/pytest, TypeScript/Jest, Rust/cargo-test, Java/JUnit, Clojure/Speclj, etc.

### Context Window Optimization

The pipeline is designed to maximize information density within a single context window:

1. **Research + Vision in one session**: Associative links are preserved in latent state
2. **GWT specs are token-efficient**: More precise than prose, fewer tokens
3. **GWT specs as resumption artifact**: If context limits are hit, the GWT file is the highest-fidelity checkpoint for a fresh session
4. **State machine as compressed representation**: The extracted graph captures the full behavioral model in a compact structure
5. **Incremental processing**: Phases can be split across sessions when necessary, with the GWT specs serving as the handoff artifact

---

## Differentiation

### vs. AWS Kiro
Kiro produces structured planning documents. Spec Engineering produces executable behavioral specifications with state machine verification. Kiro specs are consumed by AI as context; our specs are verified against the implementation.

### vs. GitHub Spec Kit
Spec Kit provides a templated four-phase workflow. Spec Engineering adds behavioral verification (GWT execution), state machine extraction, gap analysis, and progressive formal verification. Spec Kit is a workflow; Spec Engineering is a verification methodology.

### vs. Traditional BDD (Cucumber/Behave)
BDD tools map step definitions to code via regex, creating maintenance burden and requiring glue code. Spec Engineering generates complete, runnable tests from a project-aware generator. No step definitions. No glue code. The generator has full codebase knowledge.

### vs. Formal Verification Tools (TLA+/Alloy/Dafny)
These require specialized expertise and operate at either the system level (TLA+) or function level (Dafny). Spec Engineering provides a progressive on-ramp: start with behavioral specs (accessible to any developer), optionally escalate to property-based testing, and only reach for formal verification on critical paths.

### vs. C-DAD (Contract-Driven AI Development)
C-DAD proposes runtime contract enforcement. Spec Engineering operates at development time, verifying before deployment. These are complementary rather than competing: specs could evolve into runtime contracts.

---

## Risks and Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| Spec writing is a skill most developers have lost | Adoption barrier | Progressive onboarding: start with AI-assisted spec generation from existing code, refine toward behavioral purity |
| AI fills specs with implementation details (perverse incentive) | Spec quality degrades | Spec Guardian agent actively monitors and flags leakage |
| State machine extraction produces false positives/negatives | Gap analysis is noisy | Human triage of all gap analysis results; prioritize high-confidence findings |
| Parser/generator bootstrapping is project-specific overhead | Per-project setup cost | One-time cost amortized across all subsequent spec→implementation cycles |
| Formal verification adds complexity without proportional value for most code | Over-engineering | Progressive formality tiers; Tier 1 (behavioral) is always-on, Tiers 2-4 are opt-in per module |
| Context window limitations force session splitting | Information loss at session boundaries | GWT specs serve as high-fidelity resumption artifacts; state machine graph provides compressed behavioral model |
| Spec drift: implementation evolves but specs don't update | Specs become stale | CI integration: acceptance tests run on every commit; failing specs are blocking |

---

## Implementation Priorities

### Phase 1: Core Pipeline (MVP)
1. GWT authoring workflow with spec guardian
2. Parser/generator bootstrapping for 3 languages (Python, TypeScript, Rust)
3. Basic state machine extraction from GWT specs
4. CI integration for acceptance test execution

### Phase 2: Verification
1. Gap analysis engine (dead ends, unreachable states, missing error transitions)
2. Property-based test generation from GWT specs
3. Dafny annotation generation for critical paths (Tier 3)

### Phase 3: Ecosystem
1. Additional language support (Java, Go, C, Clojure)
2. Brownfield mode: extract specs from existing codebases
3. IDE integration (Claude Code plugin, VS Code extension, Cursor rules)
4. Spec diffing: visualize behavioral changes between spec versions

### Phase 4: Advanced Formal Methods
1. TLA+ generation for concurrent/distributed systems
2. Alloy model generation for structural constraints
3. Model checking integration
4. Runtime contract generation from specs (bridge to C-DAD concepts)

---

## Success Criteria

**The spec is the product. The code is derived.**

A Spec Engineering implementation is successful when:

1. A developer can describe intent, and the system produces executable behavioral specifications
2. Those specifications can be verified for completeness via state machine analysis
3. Code is generated that satisfies both behavioral and structural test streams
4. The same specifications drive implementations across multiple languages
5. Specification changes are detectable, reviewable, and CI-enforced
6. The progressive formality tiers allow teams to choose their verification depth without forcing full formal methods on every module

---

## References

### Primary Inspiration
- Robert C. Martin, Empire-2025 project and ATDD methodology (github.com/unclebob/empire-2025)
- Nick Sunny (@suny_nick), synthesis thread on Uncle Bob's GWT approach (Feb 2026)

### Formal Methods + LLMs
- Kleppmann, M. "Prediction: AI will make formal verification go mainstream" (Dec 2025)
- Congdon, B. "The Coming Need for Formal Specification" (Dec 2025)
- SpecGen: ICSE 2025, arXiv:2401.08807
- Self-Spec: OpenReview Sep 2025
- Vericoding Benchmark: arXiv:2509.22908
- NeuroInv: arXiv:2512.15816
- TLA+ Proof Automation: arXiv:2512.09758
- SysMoBench: arXiv:2509.23130

### State Machine Extraction
- FlowFSM: arXiv:2507.11222
- ProtocolGPT: arXiv:2405.00393
- SysML State Machine from Gherkin: Springer, 10.1007/s10270-024-01228-3

### Spec-Driven Development Landscape
- Thoughtworks: "Spec-driven development: Unpacking 2025's key new practices"
- Red Hat: "How spec-driven development improves AI coding quality" (Oct 2025)
- Scott Logic: "The Specification Renaissance" (Dec 2025)
- arXiv:2602.00180: "Spec-Driven Development: From Code to Contract"
- arXiv:2602.02584: "Constitutional Spec-Driven Development"

### Property-Based Testing + LLMs
- FSE 2025: "From Prompts to Properties"
- Property-Generated Solver: arXiv:2506.18315
- "Can Large Language Models Write Good Property-Based Tests?" arXiv:2307.04346

### Industry Tools
- AWS Kiro (kiro.dev)
- GitHub Spec Kit (github.com/github/spec-kit)
- OpenSpec (github.com/Fission-AI/OpenSpec)
- XState/Stately.ai
